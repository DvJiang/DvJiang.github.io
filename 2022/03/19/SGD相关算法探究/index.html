<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  <meta charset="utf-8">
  
  <title>SGD相关算法探究 | DvJiang&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/css/highlight.css">

  
  <meta name="description" content="采用数据集来自：数据竞赛：问答网站问题、回答数量预测-SofaSofa的train数据集 实现目标：用三种不同的方式自写函数预测数据questions。">
<meta property="og:type" content="article">
<meta property="og:title" content="SGD相关算法探究">
<meta property="og:url" content="http://example.com/2022/03/19/SGD%E7%9B%B8%E5%85%B3%E7%AE%97%E6%B3%95%E6%8E%A2%E7%A9%B6/index.html">
<meta property="og:site_name" content="DvJiang&#39;s Blog">
<meta property="og:description" content="采用数据集来自：数据竞赛：问答网站问题、回答数量预测-SofaSofa的train数据集 实现目标：用三种不同的方式自写函数预测数据questions。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-03-18T16:00:00.000Z">
<meta property="article:modified_time" content="2023-09-17T03:30:32.469Z">
<meta property="article:author" content="DvJiang">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary"><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="DvJiang's Blog" type="application/atom+xml">
</head>

<body>
  <div id="wrapper">
    <header id="header">
  <h1 id="title">
    <a href="/about">DvJiang</a><span>'s Blog</span>
  </h1>
  <nav>
    
    
      
      <a class="nav-link" href="/">Home</a>
    
      
        <span class="nav-spacer">×</span>
      
      <a class="nav-link" href="/archives">Archives</a>
    
      
        <span class="nav-spacer">×</span>
      
      <a class="nav-link" href="/tags">T&amp;C</a>
    
      
        <span class="nav-spacer">×</span>
      
      <a class="nav-link" href="/search">Search</a>
    

    
  </nav>
</header>

    
    <div id="content">
      <article id="post-SGD相关算法探究" class="article article-type-post" itemprop="blogPost" itemscope>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h2 class="article-title" itemprop="headline name">
      SGD相关算法探究
    </h2>
  


        <div class="article-meta">
          <time class="article-date" datetime="2022-03-18T16:00:00.000Z" itemprop="datePublished">三月 19, 2022</time>

          
          
        </div>
      </header>
    
    
        <div id="toc" class="toc-article">
    <div class="toc-title">目录</div>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#SGD%E7%AE%97%E6%B3%95"><span class="toc-text">SGD算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%B8%8E%E5%87%BD%E6%95%B0%E8%87%AA%E5%AE%9A%E4%B9%89"><span class="toc-text">参数与函数自定义</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="toc-text">相关参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0"><span class="toc-text">相关函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">注意事项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">完整代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BASGD%E7%AE%97%E6%B3%95"><span class="toc-text">随机SGD算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B-1"><span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%96%E5%80%BC%E5%90%8E%E6%94%BE%E5%9B%9E"><span class="toc-text">取值后放回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C-1"><span class="toc-text">结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%96%E5%80%BC%E5%90%8E%E4%B8%8D%E6%94%BE%E5%9B%9E"><span class="toc-text">取值后不放回</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="toc-text">代码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C-2"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-1"><span class="toc-text">完整代码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mini-batch-SGD"><span class="toc-text">Mini_batch SGD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B-2"><span class="toc-text">算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="toc-text">代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C-3"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81-2"><span class="toc-text">完整代码</span></a></li></ol></li></ol>
  </div>

      <div id="settings-container-toc">
        <div id="toc-mode">TOC</div>
      </div>
    
    
    <script type="text/javascript">
        tm = document.getElementById("toc-mode");
        let hide = localStorage.getItem("hide");
        let rr=document.documentElement.style,ff=rr.setProperty.bind(rr);
        hide = 2;
        ff('--toc-show-end','none');
        tm.onclick=()=>{
            if(hide == 1){
              ff('--toc-show-end','none');
              tm.innerHTML="TOC";
              hide = 2;
              localStorage.setItem("hide",2);
            }else{
              ff('--toc-show-end','');
              hide = 1;
              tm.innerHTML="Toc";
              localStorage.setItem("hide",1);
            }
        }
    </script>

    <div class="article-entry" itemprop="articleBody">
      
      
        <blockquote>
<p>采用数据集来自：<a target="_blank" rel="noopener" href="http://sofasofa.io/competition.php?id=4#c2">数据竞赛：问答网站问题、回答数量预测-SofaSofa</a>的train数据集</p>
<p>实现目标：用三种不同的方式自写函数预测数据questions。</p>
</blockquote>
<span id="more"></span>

<hr>
<h2 id="SGD算法"><a href="#SGD算法" class="headerlink" title="SGD算法"></a>SGD算法</h2><h3 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h3><p>所谓SGD算法，就是标准梯度下降算法，其思路就是将一个函数的梯度作为反馈进行处理，思路简单直接，是一种凸优化中常见的处理方式，在此不再加以赘述。</p>
<p>本文采用线性函数作为目标函数，选取均方差和均方根函数作为损失函数，保持学习率不变。</p>
<hr>
<h3 id="参数与函数自定义"><a href="#参数与函数自定义" class="headerlink" title="参数与函数自定义"></a>参数与函数自定义</h3><p>下列函数和参数都是自己定义的变量：</p>
<h4 id="相关参数"><a href="#相关参数" class="headerlink" title="相关参数"></a>相关参数</h4><p>设置目标函数为$y &#x3D; m_{1} x + m_{0}$,将其系数设置为一个数组m进行处理。</p>
<p>从数据集”train.csv”中取出id作为x，question作为y，路径为绝对路径，需自行更改。</p>
<p>设置学习率为定值eta（$\eta$），loss_max为损失函数变化阈值，当其变化小于这个阈值时认为不变，达到极值。</p>
<h4 id="相关函数"><a href="#相关函数" class="headerlink" title="相关函数"></a>相关函数</h4><p>优化函数：predict：用于预测数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">m, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*m[<span class="number">0</span>] + m[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>损失函数：loss：定义损失函数为均方差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>损失函数的梯度计算函数：grad：根据损失函数求导</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * np.mean(x_all * (x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all))</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * np.mean(x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br></pre></td></tr></table></figure>

<p>迭代函数：update：用于根据grad更新m取值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">eta, m, grad_loss</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.array(m) - eta * grad_loss</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ul>
<li>为使得函数更容易收敛，将x归一化。</li>
<li>同样为了使函数更容易收敛，可以改变loss为均方根，通过取根号的方式更容易收敛。</li>
<li>注意矩阵（向量）和列表的不同，有一些计算是不能使用列表的。</li>
</ul>
<hr>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">126: m: [4941.22094925  938.09814254]</span><br><span class="line">127: loss: 282856.28634899214</span><br><span class="line">127: m: [4941.42197907  937.99050829]</span><br><span class="line">128: loss: 282856.1727332602</span><br><span class="line">128: m: [4941.61243806  937.88853382]</span><br><span class="line">129: loss: 282856.07075199106</span><br><span class="line">129: m: [4941.79288205  937.79192152]</span><br><span class="line">130: loss: 282855.979213794</span><br><span class="line">130: m: [4941.96383768  937.70038943]</span><br><span class="line">m1: 2.1935037007024962</span><br><span class="line">m2: 937.7003894264082</span><br><span class="line">time cost: 0.1464390754699707</span><br></pre></td></tr></table></figure>

<p>共迭代130次，m1,m2取值分别为2.193，937.7，用时为0.14s</p>
<hr>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> ps</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义参数</span></span><br><span class="line">    m = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    eta = <span class="number">0.4</span></span><br><span class="line">    loss_max = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据</span></span><br><span class="line">    numlist = ps.read_csv(<span class="string">&#x27;XXX\\train.csv&#x27;</span>)</span><br><span class="line">    x = numlist[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    x_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    x_train = x/x_max</span><br><span class="line">    y_train = numlist[<span class="string">&#x27;questions&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    loss_last = loss(m, x_train, y_train)</span><br><span class="line">    loss_now = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-- start training ---&#x27;</span>)</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">abs</span>(loss_last - loss_now) &gt;= loss_max:</span><br><span class="line">        grad_loss = grad(m, x_train, y_train)</span><br><span class="line">        m = update(eta, m, grad_loss)</span><br><span class="line">        loss_last = loss_now</span><br><span class="line">        loss_now = loss(m, x_train, y_train)</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: loss: &#x27;</span>+<span class="built_in">str</span>(loss_now))</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: m: &#x27;</span>+<span class="built_in">str</span>(m))</span><br><span class="line">    time_end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;m1: %s \nm2: %s&#x27;</span>%(m[<span class="number">0</span>]/x_max,m[<span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;time cost: &#x27;</span>+<span class="built_in">str</span>(time_end - time_start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">m, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*m[<span class="number">0</span>] + m[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.mean((x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数的梯度计算函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * np.mean(x_all * (x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all))</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * np.mean(x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">eta, m, grad_loss</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.array(m) - eta * grad_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>其中XXX要改为自己数据所在的绝对位置</p>
<hr>
<h2 id="随机SGD算法"><a href="#随机SGD算法" class="headerlink" title="随机SGD算法"></a>随机SGD算法</h2><h3 id="算法简介-1"><a href="#算法简介-1" class="headerlink" title="算法简介"></a>算法简介</h3><p><strong>和SGD区别：</strong>由于对每一个样本求导开销往往很大，很麻烦，所以随机只选取一个样本来求导处理，这样就大大加快了运行速度。</p>
<p>因为是随机选取的，所以在期望上和原先并没有很大的不同，只是存在取值后放回和取值后不放回（不再取这个值）的区别。</p>
<p>但是，只通过一个样本就来改变模型使得结果不够稳定，常常需要迭代更多次数才能倾向于收敛（收敛得更慢了）。</p>
<hr>
<h3 id="取值后放回"><a href="#取值后放回" class="headerlink" title="取值后放回"></a>取值后放回</h3><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>改变梯度计算规则，并由于loss的计算量和原梯度计算方式的计算量相同，所以不进行这么多次的loss检查，将main中的loss改为每1000次迭代检查是否满足要求。</p>
<p>同时，为了更易收敛，将loss改为均方根，并调低了学习率。</p>
<p>改变的main为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义参数</span></span><br><span class="line">    m = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    eta = <span class="number">0.2</span></span><br><span class="line">    loss_max = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据</span></span><br><span class="line">    numlist = ps.read_csv(<span class="string">&#x27;XXX\\train.csv&#x27;</span>)</span><br><span class="line">    x = numlist[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    x_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    x_train = x/x_max</span><br><span class="line">    y_train = numlist[<span class="string">&#x27;questions&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    loss_last = loss(m, x_train, y_train)</span><br><span class="line">    loss_now = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-- start training ---&#x27;</span>)</span><br><span class="line">    num = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">abs</span>(loss_last - loss_now) &gt;= loss_max:</span><br><span class="line">        <span class="keyword">while</span> num % <span class="number">1000</span> != <span class="number">0</span>:</span><br><span class="line">            grad_loss = grad_yes(m, x_train, y_train)</span><br><span class="line">            m = update(eta, m, grad_loss)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        loss_last = loss_now</span><br><span class="line">        loss_now = loss(m, x_train, y_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: loss: &#x27;</span>+<span class="built_in">str</span>(loss_now))</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: m: &#x27;</span>+<span class="built_in">str</span>(m))</span><br><span class="line">    time_end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;m1: %s \nm2: %s&#x27;</span>%(m[<span class="number">0</span>]/x_max,m[<span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;time cost: &#x27;</span>+<span class="built_in">str</span>(time_end - time_start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br></pre></td></tr></table></figure>

<p>而改变的grad为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_yes</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    n = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(x_all))</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * x_all[n] * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br></pre></td></tr></table></figure>

<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">30001: m: [4574.0792231  1459.14024352]</span><br><span class="line">31001: loss: 693.3256213246475</span><br><span class="line">31001: m: [5359.3018607 1157.2512358]</span><br><span class="line">32001: loss: 667.4950871908976</span><br><span class="line">32001: m: [5372.91711498 1105.99657984]</span><br><span class="line">33001: loss: 548.5828585929333</span><br><span class="line">33001: m: [4956.27599193 1064.88519278]</span><br><span class="line">34001: loss: 548.5393614856855</span><br><span class="line">34001: m: [4522.56052786 1091.12039757]</span><br><span class="line">m1: 2.0073504340254114</span><br><span class="line">m2: 1091.120397570372</span><br><span class="line">time cost: 0.5240442752838135</span><br></pre></td></tr></table></figure>

<p>34000次迭代后满足要求，可看出计算开销变小，但更不易收敛。</p>
<hr>
<h3 id="取值后不放回"><a href="#取值后不放回" class="headerlink" title="取值后不放回"></a>取值后不放回</h3><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p>同放回的处理，只是添加一个全局变量用于记录是否被取值。</p>
<p>grad改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_no</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">global</span> x_take</span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(x_all))</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> x_take:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * x_all[n] * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br></pre></td></tr></table></figure>

<h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">248001: m: [4880.01644085  840.39996252]</span><br><span class="line">249001: loss: 628.2718124284057</span><br><span class="line">249001: m: [5063.90178143  543.89033753]</span><br><span class="line">250001: loss: 744.7237441508288</span><br><span class="line">250001: m: [4868.60609451  453.4499488 ]</span><br><span class="line">251001: loss: 546.3392904223941</span><br><span class="line">251001: m: [5322.36473068  685.92958607]</span><br><span class="line">252001: loss: 546.3997052005344</span><br><span class="line">252001: m: [5335.40219647  795.55018315]</span><br><span class="line">m1: 2.368132355291041</span><br><span class="line">m2: 795.5501831533912</span><br><span class="line">time cost: 3.8524723052978516</span><br></pre></td></tr></table></figure>

<p>可以看到时间变长，计算不稳定。</p>
<h4 id="完整代码-1"><a href="#完整代码-1" class="headerlink" title="完整代码"></a>完整代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> ps</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">x_take = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义参数</span></span><br><span class="line">    m = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    eta = <span class="number">0.2</span></span><br><span class="line">    loss_max = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据</span></span><br><span class="line">    numlist = ps.read_csv(<span class="string">&#x27;XXX\\train.csv&#x27;</span>)</span><br><span class="line">    x = numlist[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    x_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    x_train = x/x_max</span><br><span class="line">    y_train = numlist[<span class="string">&#x27;questions&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    loss_last = loss(m, x_train, y_train)</span><br><span class="line">    loss_now = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-- start training ---&#x27;</span>)</span><br><span class="line">    num = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">abs</span>(loss_last - loss_now) &gt;= loss_max:</span><br><span class="line">        <span class="keyword">while</span> num % <span class="number">1000</span> != <span class="number">0</span>:</span><br><span class="line">            grad_loss = grad_yes(m, x_train, y_train)</span><br><span class="line">            m = update(eta, m, grad_loss)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        loss_last = loss_now</span><br><span class="line">        loss_now = loss(m, x_train, y_train)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: loss: &#x27;</span>+<span class="built_in">str</span>(loss_now))</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: m: &#x27;</span>+<span class="built_in">str</span>(m))</span><br><span class="line">    time_end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;m1: %s \nm2: %s&#x27;</span>%(m[<span class="number">0</span>]/x_max,m[<span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;time cost: &#x27;</span>+<span class="built_in">str</span>(time_end - time_start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">m, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*m[<span class="number">0</span>] + m[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.mean((x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数的梯度计算函数（有放回）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_yes</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    n = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(x_all))</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * x_all[n] * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数的梯度计算函数（无放回）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_no</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">global</span> x_take</span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        n = np.random.randint(<span class="number">0</span>,<span class="built_in">len</span>(x_all))</span><br><span class="line">        <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> x_take:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * x_all[n] * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * (x_all[n] * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all[n])</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">eta, m, grad_loss</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.array(m) - eta * grad_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>



<hr>
<h2 id="Mini-batch-SGD"><a href="#Mini-batch-SGD" class="headerlink" title="Mini_batch SGD"></a>Mini_batch SGD</h2><h3 id="算法简介-2"><a href="#算法简介-2" class="headerlink" title="算法简介"></a>算法简介</h3><p>小批量SGD综合了前两个方法的优点，它采用随机选取小批量数据的方式，使得结果平衡了一次梯度计算开销和迭代次数（稳定性）。通过合理选取一次计算的样本个数batch，我们可以得到一个比较综合的结果</p>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p>main的处理与随机SGD类似，都是等循环多次迭代后才检测是否满足要求，只是改变了grad的处理方式。</p>
<p>添加rand_int函数作为随机生成关于x的数组的函数，其定义方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randon_int</span>(<span class="params">n, x_all, y_all</span>):</span></span><br><span class="line">    x_rand = []</span><br><span class="line">    y_rand = []</span><br><span class="line">    <span class="keyword">while</span> n &gt;= <span class="number">1</span>:</span><br><span class="line">        n -= <span class="number">1</span></span><br><span class="line">        num_x = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(x_all))</span><br><span class="line">        x_rand.append(x_all[num_x])</span><br><span class="line">        y_rand.append(y_all[num_x])</span><br><span class="line">    <span class="keyword">return</span> np.array(x_rand), np.array(y_rand)</span><br></pre></td></tr></table></figure>

<p>将此函数生成的数组替代原先的数组，重写grad函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">m, x_all, y_all, batch</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    x_rand, y_rand = randon_int(batch, x_all, y_all)</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * np.mean(x_rand * (x_rand * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_rand))</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * np.mean(x_rand * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_rand)</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br></pre></td></tr></table></figure>

<h3 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="number">1602</span>: m: [<span class="number">4968.82514484</span>  <span class="number">877.14162992</span>]</span><br><span class="line"><span class="number">1702</span>: loss: <span class="number">532.5291872425305</span></span><br><span class="line"><span class="number">1702</span>: m: [<span class="number">4854.68393389</span>  <span class="number">988.44482143</span>]</span><br><span class="line"><span class="number">1802</span>: loss: <span class="number">532.7930688072918</span></span><br><span class="line"><span class="number">1802</span>: m: [<span class="number">4860.3578364</span>   <span class="number">958.02605338</span>]</span><br><span class="line"><span class="number">1902</span>: loss: <span class="number">532.5027597249214</span></span><br><span class="line"><span class="number">1902</span>: m: [<span class="number">4896.79865359</span>  <span class="number">937.60044292</span>]</span><br><span class="line"><span class="number">2002</span>: loss: <span class="number">532.5811608417331</span></span><br><span class="line"><span class="number">2002</span>: m: [<span class="number">4916.35037578</span>  <span class="number">977.216822</span>  ]</span><br><span class="line">m1: <span class="number">2.182135097993372</span></span><br><span class="line">m2: <span class="number">977.2168220048245</span></span><br><span class="line">time cost: <span class="number">0.1969621181488037</span></span><br></pre></td></tr></table></figure>

<p>尽管迭代了2000次，其时间消耗仍然较小，结果也比较稳定。</p>
<h3 id="完整代码-2"><a href="#完整代码-2" class="headerlink" title="完整代码"></a>完整代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> ps</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    time_start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义参数</span></span><br><span class="line">    m = [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    eta = <span class="number">0.1</span></span><br><span class="line">    loss_max = <span class="number">0.1</span></span><br><span class="line">    batch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分数据</span></span><br><span class="line">    numlist = ps.read_csv(<span class="string">&#x27;XXX\\train.csv&#x27;</span>)</span><br><span class="line">    x = numlist[<span class="string">&#x27;id&#x27;</span>]</span><br><span class="line">    x_max = np.<span class="built_in">max</span>(x)</span><br><span class="line">    x_train = x/x_max</span><br><span class="line">    y_train = numlist[<span class="string">&#x27;questions&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    loss_last = loss(m, x_train, y_train)</span><br><span class="line">    loss_now = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-- start training ---&#x27;</span>)</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> np.<span class="built_in">abs</span>(loss_last - loss_now) &gt;= loss_max:</span><br><span class="line">        <span class="keyword">while</span> num % <span class="number">100</span> != <span class="number">0</span>:</span><br><span class="line">            grad_loss = grad(m, x_train, y_train, batch)</span><br><span class="line">            m = update(eta, m, grad_loss)</span><br><span class="line">            num += <span class="number">1</span></span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        loss_last = loss_now</span><br><span class="line">        loss_now = loss(m, x_train, y_train)</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: loss: &#x27;</span>+<span class="built_in">str</span>(loss_now))</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">str</span>(num) + <span class="string">&#x27;: m: &#x27;</span>+<span class="built_in">str</span>(m))</span><br><span class="line">    time_end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;m1: %s \nm2: %s&#x27;</span>%(m[<span class="number">0</span>]/x_max,m[<span class="number">1</span>]))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;time cost: &#x27;</span>+<span class="built_in">str</span>(time_end - time_start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">m, x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x*m[<span class="number">0</span>] + m[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">m, x_all, y_all</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.mean((x_all * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_all)**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义生成x的随机子数组</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randon_int</span>(<span class="params">n, x_all, y_all</span>):</span></span><br><span class="line">    x_rand = []</span><br><span class="line">    y_rand = []</span><br><span class="line">    <span class="keyword">while</span> n &gt;= <span class="number">1</span>:</span><br><span class="line">        n -= <span class="number">1</span></span><br><span class="line">        num_x = np.random.randint(<span class="number">0</span>, <span class="built_in">len</span>(x_all))</span><br><span class="line">        x_rand.append(x_all[num_x])</span><br><span class="line">        y_rand.append(y_all[num_x])</span><br><span class="line">    <span class="keyword">return</span> np.array(x_rand), np.array(y_rand)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数的梯度计算函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span>(<span class="params">m, x_all, y_all, batch</span>):</span></span><br><span class="line">    grad_loss = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    x_rand, y_rand = randon_int(batch, x_all, y_all)</span><br><span class="line">    grad_loss[<span class="number">0</span>] = <span class="number">2</span> * np.mean(x_rand * (x_rand * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_rand))</span><br><span class="line">    grad_loss[<span class="number">1</span>] = <span class="number">2</span> * np.mean(x_rand * m[<span class="number">0</span>] + m[<span class="number">1</span>] - y_rand)</span><br><span class="line">    <span class="keyword">return</span> np.array(grad_loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">eta, m, grad_loss</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.array(m) - eta * grad_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    <div class="article-category">
      
        <b>分类:</b>
        <a class="article-category-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a>
      
      
        <br/>
      
      
        <b>标签:</b>
        <a class="article-tag-none-link" href="/tags/Python/" rel="tag">Python</a>, <a class="article-tag-none-link" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag">学习</a>, <a class="article-tag-none-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
      
    </div>
    
    
  </div>
</article>

  
<nav id="article-nav" class="article-nav">
  
    <a href="/2022/03/23/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          线性代数的本质
        
      </div>
    </a>
  
  
    <a href="/2022/03/17/%E3%80%8A%E5%A1%9E%E5%B0%94%E8%BE%BE%E4%BC%A0%E8%AF%B4%EF%BC%9A%E6%97%B7%E9%87%8E%E4%B9%8B%E6%81%AF%E3%80%8B%E4%B8%80%E5%9C%BA%E5%AD%A4%E7%8B%AC%E7%9A%84%E6%97%85%E8%A1%8C/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          《塞尔达传说：旷野之息》一场孤独的旅行
        
      </div>
    </a>
  
</nav>






    </div>
  </div>
  




<div id="settings-container">
  <div id="dark-mode">DAR</div>
  <div id="sans-font">SON</div>
</div>
<script type="text/javascript">
let d=document,r=d.documentElement.style,f=r.setProperty.bind(r),l=localStorage,s=l.getItem('s')||
(window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
,n=l.getItem('n'),m=d.getElementById("dark-mode"),
b=()=>{f('--bg-color','#fafafa');f('--code-bg-color','#f4f4f4');f('--text-color','#212121');f('--secondary-color','#808080');f('--tertiary-color','#b0b0b0');f('--link-color','#b5c8cf');f('--link-hover-color','#618794');f('--link-bg-color','#dae4e7');f('--selection-color','#dae4e7');m.innerHTML="DAR"},
c=()=>{f('--bg-color','#212121');f('--code-bg-color','#292929');f('--text-color','#fff');f('--secondary-color','#c0c0c0');f('--tertiary-color','#6e6e6e');f('--link-color','#4d6b75');f('--link-hover-color','#96b1bb');f('--link-bg-color','#5d828e');f('--selection-color','#acc1c9');m.innerHTML="LIG"},
o=d.getElementById("sans-font"),
e=()=>{f('--body-stack',' "Lora", "Georgia", "Times New Roman", serif');o.innerHTML="HEI"},
g=()=>{f('--body-stack',' "Lato", "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", "Verdana", sans-serif');o.innerHTML="SON"};
m.onclick=()=>{if(s==2){s=1;l.setItem('s',s);c()}else{s=2;l.setItem('s',s);b()}};
o.onclick=()=>{if(n==2){n=1;l.setItem('n',n);g()}else{n=2;l.setItem('n',n);e()}};
if(!s){s=1;l.setItem('s',1)};if(s==1){c()};
if(!n){n=1;l.setItem('n',1)};if(n==1){g()};
</script>


<div class = "page-nav">
  ©2022 - 2023 , content by DvJiang. All Rights Reserved.
</div>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div id="busuanzi_container_site_pv" class = "page-nav">
Total site vist : <span id="busuanzi_value_site_pv"></span> |
Total visitor : <span id="busuanzi_value_site_uv"></span> | 
Total page visit : <span id="busuanzi_value_page_pv"></span>
</div>







</body>
</html>
